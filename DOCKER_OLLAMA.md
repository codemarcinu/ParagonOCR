# ğŸ³ ParagonWeb + Ollama w Dockerze

## PrzeglÄ…d

ParagonWeb moÅ¼e dziaÅ‚aÄ‡ w dwÃ³ch trybach:
1. **Cloud** (domyÅ›lny) - OpenAI + Mistral OCR
2. **Lokalny** - Ollama + Tesseract

W trybie lokalnym, Ollama dziaÅ‚a w osobnym kontenerze Docker i komunikuje siÄ™ z ParagonWeb przez sieÄ‡ Docker.

## Architektura Docker

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Docker Network                  â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ParagonWeb  â”‚â”€â”€â”€â–¶â”‚   Ollama     â”‚ â”‚
â”‚  â”‚  Container   â”‚    â”‚  Container   â”‚ â”‚
â”‚  â”‚  :8000, :8080â”‚    â”‚   :11434     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Konfiguracja

### Automatyczna konfiguracja

W Dockerze, `OLLAMA_HOST` jest automatycznie ustawiane na `http://ollama:11434` (nazwa serwisu Docker).

**Nie musisz tego konfigurowaÄ‡ rÄ™cznie!**

### RÄ™czna konfiguracja (opcjonalnie)

JeÅ›li chcesz uÅ¼yÄ‡ zewnÄ™trznego Ollama (poza Dockerem):

```yaml
# docker-compose.yml
services:
  paragon-web:
    environment:
      - OLLAMA_HOST=http://host.docker.internal:11434  # Windows/Mac
      # lub
      - OLLAMA_HOST=http://172.17.0.1:11434  # Linux (docker0 bridge)
```

## Uruchomienie

### Tryb Cloud (domyÅ›lny)

```bash
docker-compose up -d
```

Kontener Ollama jest uruchamiany, ale nie jest uÅ¼ywany (chyba Å¼e przeÅ‚Ä…czysz na tryb lokalny).

### Tryb Lokalny

```bash
# UÅ¼yj specjalnego docker-compose dla trybu lokalnego
docker-compose -f docker-compose.local.yml up -d
```

## Pobieranie modeli Ollama

### Automatyczne

Modele sÄ… pobierane automatycznie przy pierwszym uÅ¼yciu. To moÅ¼e zajÄ…Ä‡ kilka minut.

### RÄ™czne

```bash
# PoÅ‚Ä…cz siÄ™ z kontenerem Ollama
docker exec -it paragon_ollama ollama pull llava:latest
docker exec -it paragon_ollama ollama pull SpeakLeash/bielik-11b-v2.3-instruct:Q4_K_M

# SprawdÅº pobrane modele
docker exec -it paragon_ollama ollama list
```

## Sprawdzanie statusu

### Czy Ollama dziaÅ‚a?

```bash
# SprawdÅº kontener
docker ps | grep ollama

# SprawdÅº logi
docker logs paragon_ollama

# SprawdÅº API
docker exec paragon_ollama curl http://localhost:11434/api/tags
```

### Czy ParagonWeb Å‚Ä…czy siÄ™ z Ollama?

```bash
# SprawdÅº logi ParagonWeb
docker logs paragon_ocr | grep -i ollama

# SprawdÅº konfiguracjÄ™
docker exec paragon_ocr env | grep OLLAMA
```

## Troubleshooting

### Problem: "Nie moÅ¼na poÅ‚Ä…czyÄ‡ siÄ™ z Ollama"

**RozwiÄ…zanie:**
1. SprawdÅº czy kontener Ollama dziaÅ‚a:
   ```bash
   docker ps | grep ollama
   ```

2. SprawdÅº czy sÄ… w tej samej sieci:
   ```bash
   docker network inspect paragonocr_default
   ```

3. SprawdÅº OLLAMA_HOST w ParagonWeb:
   ```bash
   docker exec paragon_ocr env | grep OLLAMA_HOST
   # Powinno byÄ‡: OLLAMA_HOST=http://ollama:11434
   ```

### Problem: "Model nie znaleziony"

**RozwiÄ…zanie:**
```bash
# Pobierz model rÄ™cznie
docker exec -it paragon_ollama ollama pull llava:latest

# SprawdÅº dostÄ™pne modele
docker exec -it paragon_ollama ollama list
```

### Problem: "Wolne przetwarzanie"

**RozwiÄ…zanie:**
- Ollama w Dockerze moÅ¼e byÄ‡ wolne na sÅ‚abszym sprzÄ™cie
- RozwaÅ¼ uÅ¼ycie trybu Cloud (OpenAI jest szybsze)
- Lub zwiÄ™ksz zasoby dla kontenera Ollama:
  ```yaml
  ollama:
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
  ```

## PrzeÅ‚Ä…czanie miÄ™dzy trybami

### Z Cloud na Lokalny

1. Zatrzymaj kontenery:
   ```bash
   docker-compose down
   ```

2. Uruchom z konfiguracjÄ… lokalnÄ…:
   ```bash
   docker-compose -f docker-compose.local.yml up -d
   ```

3. Upewnij siÄ™, Å¼e modele sÄ… pobrane (patrz wyÅ¼ej)

### Z Lokalnego na Cloud

1. Zatrzymaj kontenery:
   ```bash
   docker-compose -f docker-compose.local.yml down
   ```

2. Uruchom standardowy docker-compose:
   ```bash
   docker-compose up -d
   ```

## Volume dla modeli Ollama

Modele Ollama sÄ… przechowywane w volume `ollama_data`, wiÄ™c sÄ… zachowywane miÄ™dzy restartami:

```bash
# SprawdÅº volume
docker volume ls | grep ollama

# Backup modeli (opcjonalnie)
docker run --rm -v ollama_data:/data -v $(pwd):/backup alpine tar czf /backup/ollama_backup.tar.gz /data
```

## WydajnoÅ›Ä‡

### Zalecenia

- **Cloud (OpenAI):** Najszybsze, najlepsze dla produkcji
- **Lokalny (Ollama):** Wymaga mocnego sprzÄ™tu (GPU zalecane)
- **Hybrydowy:** Cloud OCR + Lokalny AI (kompromis)

### Zasoby dla Ollama

Minimalne:
- 4GB RAM
- 2 CPU cores
- 10GB miejsca na dysku (dla modeli)

Zalecane:
- 8GB+ RAM
- 4+ CPU cores
- GPU (CUDA) dla szybszego przetwarzania
- 20GB+ miejsca na dysku

## BezpieczeÅ„stwo

- Ollama w Dockerze jest izolowane od hosta
- Komunikacja miÄ™dzy kontenerami odbywa siÄ™ przez sieÄ‡ Docker (nie jest eksponowana na zewnÄ…trz)
- Port 11434 jest eksponowany tylko jeÅ›li chcesz uÅ¼yÄ‡ Ollama z hosta

## Przydatne komendy

```bash
# Restart Ollama
docker restart paragon_ollama

# WyczyÅ›Ä‡ cache Ollama
docker exec paragon_ollama ollama rm <model_name>

# SprawdÅº uÅ¼ycie zasobÃ³w
docker stats paragon_ollama

# Zobacz wszystkie modele
docker exec -it paragon_ollama ollama list

# UsuÅ„ wszystkie modele (ostroÅ¼nie!)
docker exec -it paragon_ollama ollama list | awk '{print $1}' | xargs -I {} docker exec paragon_ollama ollama rm {}
```

---

**Ostatnia aktualizacja:** 2025-11-23

