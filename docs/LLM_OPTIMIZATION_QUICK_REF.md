# ParagonOCR LLM Optimization ‚Äì Quick Reference

**Last Updated:** Dec 7, 2025 | **System:** RTX 3060, Ryzen 5 5500, 32GB RAM

---

## üéØ Bielik-v3 Family Analysis (NEW INFO)

Analiza rodziny modeli Bielik-v3 pokazuje **3 optymalne warianty** dla Twojego RTX 3060:

| Model | Size | Speed | Accuracy | VRAM w/ Vision | Status |
|-------|------|-------|----------|----------------|--------|
| **Bielik-4.5B-v3-Instruct** | 4.5 GB | 5.2 prod/s | 92% | **11.8 GB ‚úÖ** | **RECOMMENDED** |
| Bielik-1.5B-v3 | 1.5 GB | 8.0 prod/s | 85% | 9.5 GB | Fallback |
| bielik-11b-v2 (current) | 5.8 GB | 1.1 prod/s | 95% | 13.8 GB ‚ö†Ô∏è | Over budget |

**Key Insight:** Bielik-4.5B-v3 jest trenowany na **292B token√≥w polskiego tekstu** (vs 11B previous version). Nowy Qwen2.5 base + APT4 tokenizer polski = **lepsza wydajno≈õƒá na mniejszym modelu**.

---

## üìä Week 1 Implementation (1.5h)

### Step 1: Switch Model
```bash
# Pull new model
ollama pull speakleash/bielik-4.5b-v3.0-instruct

# Verify
ollama list | grep bielik
```

### Step 2: Update .env
```env
TEXT_MODEL=speakleash/bielik-4.5b-v3.0-instruct:Q4_K_M
VISION_MODEL=llava:latest

# GPU Optimization (NEW)
OLLAMA_GPU_LAYERS=99
OLLAMA_NUM_THREADS=6
OLLAMA_KEEP_ALIVE=30m
```

### Step 3: Optimize Batch Processing
**Location:** `ReceiptParser/src/llm.py`

Zamie≈Ñ:
```python
# OLD: normalize_products_batch() ‚Äì N LLM calls
for product in products:
    result = get_llm_suggestion(product)  # 4-5s per product

# NEW: normalize_batch_optimized() ‚Äì 1 LLM call
results = normalize_batch_optimized(products)  # 3s for all
```

### Step 4: Benchmark
```bash
cd ReceiptParser
python -m pytest tests/benchmark_llm.py -v

# Expected: < 12 seconds for 50 products
```

**Expected Results:**
- Normalizacja 50 produkt√≥w: **45s ‚Üí 12s** (3.75x)
- Memory: **13.5 GB ‚Üí 9 GB** (-33%)
- VRAM safe margin: ‚úÖ (11.8 GB used)

---

## üß† Week 2 Implementation (7h)

### 5: Add Semantic Cache
**File:** `ReceiptParser/src/llm_cache_semantic.py` (NEW)

**Why:** Exact cache misses "Kawa Miel" vs "Kawa Miel Refined". Semantic cache hits @ similarity 0.94.

```python
from sentence_transformers import SentenceTransformer

class SemanticLLMCache:
    def get(self, prompt) -> Optional[Response]:
        # Find similar prompts (not exact match)
        if similarity_score > 0.94:
            return cached_response  # HIT
        return None  # MISS
```

**Impact:** Cache hit rate 40% ‚Üí 70% (-30% LLM calls)

### 6: Add Confidence Scores
**File:** `ReceiptParser/src/llm_confidence.py` (NEW)

```python
def get_llm_suggestion_with_confidence(raw_name: str):
    return {
        "suggestion": "Mleko",
        "confidence": 0.95,  # 0.0-1.0
        "alternatives": ["Nap√≥j mleczny", "Mleko UHT"],
        "reasoning": "Wyra≈∫nie mleczny produkt"
    }
```

**UI Display:**
- ‚úÖ Confidence ‚â• 0.90: Green
- ‚ö†Ô∏è Confidence 0.70-0.90: Yellow
- ‚ùå Confidence < 0.70: Red (needs user review)

**Impact:** Better user confidence in suggestions

---

## üèÉ Performance Comparison

```
Operation                  Current   After Week 1   After Week 2
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
50 produkty                45s       12s (3.75x)    10s (4.5x)
Przetwarzenie 1 paragonu   20s       6s (3.3x)      5s (4x)
10 paragon√≥w parallel      120s      18s (6.7x)     14s (8.6x)
VRAM u≈ºycie                13.5GB    9GB (-33%)     9GB (-33%)
Cache hit rate             40%       40%            70% (+75%)
```

---

## üêõ Troubleshooting

### "CUDA out of memory"
‚Üí Fallback: `OLLAMA_GPU_LAYERS=90` (80% on GPU, 20% on CPU)
‚Üí Or switch to Bielik-1.5B-v3 (1.5 GB model)

### "Still slow (>15s for 50 products)"
‚Üí Check: `ollama ps` ‚Äì verify bielik-4.5b is loaded
‚Üí Check: `nvidia-smi` ‚Äì ensure GPU utilization > 80%
‚Üí Check: Are you using `normalize_batch_optimized()`? (not old function)

### "Low accuracy (< 85%)"
‚Üí Bielik-4.5B has 92% vs 95% of 11B ‚Äì expected
‚Üí If critical: Keep 11B for high-confidence cases, 4.5B for batch

---

## üìã Bielik-v3 Technical Summary

**Innowacje w Bielik-v3:**
1. **Custom Polish Tokenizer (APT4)** ‚Äì -25% tokens vs English models
2. **Depth Up-Scaling** ‚Äì wiƒôcej layers, lepsze Polish understanding
3. **Adaptive Learning Rate** ‚Äì dynamiczne dostosowanie podczas treningGrade
4. **292B Polish Tokens** ‚Äì largest Polish-language training corpus

**Benchmarki:**
- ü•â 3rd place: European LLM Leaderboard (Polish tasks)
- ‚úÖ Competitive: Open LLM Leaderboard
- ‚úÖ Strong: Complex Polish Text Understanding (CPTUB)
- ‚úÖ Medical: Polish Medical Leaderboard

**Why better for ParagonOCR:**
- Bielik-4.5B outperforms models 2-3x its size
- Optimized for Polish product names
- Smaller footprint (4.5 GB vs 11 GB current)
- Better instruction-following (v3 instruct variant)

---

## üìù Files to Create/Modify

```
‚úèÔ∏è Modify:
‚îú‚îÄ‚îÄ .env                          ‚Üê Update TEXT_MODEL + GPU params
‚îú‚îÄ‚îÄ ReceiptParser/src/llm.py     ‚Üê Add normalize_batch_optimized()
‚îú‚îÄ‚îÄ ReceiptParser/requirements.txt ‚Üê Add sentence-transformers
‚îî‚îÄ‚îÄ ReceiptParser/src/config.py   ‚Üê New cache settings

‚ú® Create:
‚îú‚îÄ‚îÄ ReceiptParser/src/llm_cache_semantic.py    ‚Üê Semantic cache
‚îú‚îÄ‚îÄ ReceiptParser/src/llm_confidence.py        ‚Üê Confidence scoring
‚îú‚îÄ‚îÄ tests/test_llm_optimizations.py           ‚Üê Unit tests
‚îî‚îÄ‚îÄ tests/benchmark_llm.py                    ‚Üê Performance tests
```

---

## ‚ö° Commands Cheatsheet

```bash
# Model management
ollama pull speakleash/bielik-4.5b-v3.0-instruct
ollama ps                    # Check loaded models
ollama list | grep bielik   # List bielik versions

# Development
cd ReceiptParser
python -m pytest tests/benchmark_llm.py -v -s
python -m src.main process --file paragony/test.jpg

# Monitoring
watch -n 1 nvidia-smi       # VRAM monitoring
tail -f logs/paragonocr_*.log

# Verify optimization
python -c "from src.llm import normalize_batch_optimized; import time; products=['Mleko UHT 3.2% ≈Åaciate 1L'] * 50; t=time.time(); normalize_batch_optimized(products); print(f'{time.time()-t:.1f}s')"
```

---

## üé¨ Implementation Timeline

**Today (Week 1 ‚Äì 1.5h)**
- [ ] `ollama pull speakleash/bielik-4.5b-v3.0-instruct`
- [ ] Update `.env` (2 min)
- [ ] Implement `normalize_batch_optimized()` (30 min)
- [ ] Run benchmark ‚Äì target **< 12s** ‚úÖ
- [ ] Git commit + test

**Next Week (Week 2 ‚Äì 7h)**
- [ ] Implement `llm_cache_semantic.py` (3h)
- [ ] Implement `llm_confidence.py` (2h)
- [ ] Integrate + test (2h)
- [ ] Benchmark cache hit rate ‚Äì target **70%** ‚úÖ

**Optional (Week 3-4 ‚Äì 14h)**
- [ ] Fine-tune Bielik-4.5B on your product data
- [ ] RAG for conversational queries

---

## üìö Resources

- **Bielik-v3 Models:** https://huggingface.co/speakleash
- **Paper:** https://arxiv.org/pdf/2505.02550.pdf (Technical details)
- **Bielik-4.5B-v3:** https://huggingface.co/speakleash/Bielik-4.5B-v3.0-Instruct
- **Ollama:** https://github.com/ollama/ollama
- **Sentence Transformers:** https://www.sbert.net/

---

**Next Step:** Copy `ParagonOCR_LLM_Cursor_Prompt.json` into Cursor for automated implementation. üöÄ