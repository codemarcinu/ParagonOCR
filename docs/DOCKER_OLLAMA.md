# ğŸ³ ParagonWeb + Ollama w Dockerze

## PrzeglÄ…d

ParagonWeb moÅ¼e dziaÅ‚aÄ‡ w dwÃ³ch trybach:
1. **Cloud** (domyÅ›lny) - OpenAI + Mistral OCR
2. **Lokalny** - Ollama + Tesseract

W trybie lokalnym, ParagonWeb moÅ¼e uÅ¼ywaÄ‡ Ollama na kilka sposobÃ³w:
- **IstniejÄ…cy kontener Ollama** (zalecane) - jeÅ›li masz juÅ¼ uruchomiony Ollama
- **Nowy kontener Ollama** - jeÅ›li potrzebujesz osobnego kontenera dla tego projektu
- **Ollama na hoÅ›cie** - jeÅ›li Ollama dziaÅ‚a bezpoÅ›rednio na systemie (poza Dockerem)

## âš ï¸ WaÅ¼ne: Nie twÃ³rz drugiego kontenera Ollama!

JeÅ›li masz juÅ¼ uruchomiony kontener Ollama (np. systemowy), **NIE TWÃ“RZ DRUGIEGO**! 
UÅ¼yj istniejÄ…cego, ustawiajÄ…c odpowiedni `OLLAMA_HOST` w docker-compose.yml.

## Architektura Docker

### Opcja 1: IstniejÄ…cy kontener Ollama (zalecane)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Docker Network                  â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  ParagonWeb  â”‚â”€â”€â”€â–¶ (istniejÄ…cy)     â”‚
â”‚  â”‚  Container   â”‚    Ollama Container  â”‚
â”‚  â”‚  :8000, :8080â”‚    :11434            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Opcja 2: Nowy kontener Ollama (tylko jeÅ›li potrzebny)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Docker Network                  â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ParagonWeb  â”‚â”€â”€â”€â–¶â”‚   Ollama     â”‚ â”‚
â”‚  â”‚  Container   â”‚    â”‚  Container   â”‚ â”‚
â”‚  â”‚  :8000, :8080â”‚    â”‚   :11434     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Opcja 3: Ollama na hoÅ›cie
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Docker Network                  â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  ParagonWeb  â”‚â”€â”€â”€â–¶ host.docker.     â”‚
â”‚  â”‚  Container   â”‚    internal:11434    â”‚
â”‚  â”‚  :8000, :8080â”‚    (Ollama na hoÅ›cie)â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Konfiguracja

### UÅ¼ycie istniejÄ…cego kontenera Ollama

JeÅ›li masz juÅ¼ uruchomiony kontener Ollama, znajdÅº jego nazwÄ™ lub IP:

```bash
# SprawdÅº uruchomione kontenery Ollama
docker ps | grep ollama

# SprawdÅº IP kontenera
docker inspect <nazwa_kontenera> | grep IPAddress
```

NastÄ™pnie w `docker-compose.yml` ustaw:

```yaml
services:
  paragon-web:
    environment:
      # JeÅ›li kontener jest w tej samej sieci Docker:
      - OLLAMA_HOST=http://<nazwa_kontenera>:11434
      # Lub uÅ¼yj IP kontenera:
      - OLLAMA_HOST=http://<IP_kontenera>:11434
```

**WAÅ»NE:** Upewnij siÄ™, Å¼e oba kontenery sÄ… w tej samej sieci Docker:
```bash
# SprawdÅº sieÄ‡ istniejÄ…cego kontenera
docker inspect <nazwa_kontenera> | grep NetworkMode

# JeÅ›li sÄ… w rÃ³Å¼nych sieciach, poÅ‚Ä…cz je:
docker network connect <nazwa_sieci> <nazwa_kontenera_ollama>
```

### UÅ¼ycie Ollama na hoÅ›cie (poza Dockerem)

JeÅ›li Ollama dziaÅ‚a bezpoÅ›rednio na systemie:

```yaml
# docker-compose.yml
services:
  paragon-web:
    environment:
      - OLLAMA_HOST=http://host.docker.internal:11434  # Windows/Mac
      # lub dla Linuxa (sprawdÅº IP mostu docker0):
      - OLLAMA_HOST=http://172.17.0.1:11434  # Linux (docker0 bridge)
```

Aby znaleÅºÄ‡ IP mostu docker0 na Linuxie:
```bash
ip addr show docker0 | grep inet
```

### Utworzenie nowego kontenera Ollama (tylko jeÅ›li potrzebny)

JeÅ›li naprawdÄ™ potrzebujesz nowego kontenera, odkomentuj serwis `ollama` w `docker-compose.yml`:
```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: paragon_ollama
    # ... reszta konfiguracji
```

## Sprawdzanie istniejÄ…cego Ollama

**PRZED uruchomieniem ParagonWeb, sprawdÅº czy masz juÅ¼ Ollama:**

```bash
# SprawdÅº kontenery Docker z Ollama
docker ps -a | grep ollama

# SprawdÅº czy Ollama dziaÅ‚a na hoÅ›cie (port 11434)
curl http://localhost:11434/api/tags

# SprawdÅº wszystkie kontenery Ollama (rÃ³wnieÅ¼ zatrzymane)
docker ps -a --filter "ancestor=ollama/ollama"
```

JeÅ›li masz juÅ¼ Ollama:
1. **Kontener Docker:** Ustaw `OLLAMA_HOST` na nazwÄ™ kontenera lub jego IP (patrz sekcja "Konfiguracja" wyÅ¼ej)
2. **Na hoÅ›cie:** Ustaw `OLLAMA_HOST` na `http://host.docker.internal:11434` (Mac/Windows) lub `http://172.17.0.1:11434` (Linux)
3. **NIE TWÃ“RZ** nowego kontenera Ollama w docker-compose.yml!

## Uruchomienie

### Tryb Cloud (domyÅ›lny)

```bash
docker-compose up -d
```

W trybie Cloud, Ollama nie jest uÅ¼ywane (nawet jeÅ›li jest uruchomione).

### Tryb Lokalny

**JeÅ›li masz juÅ¼ Ollama:**
1. Ustaw `OLLAMA_HOST` w `docker-compose.local.yml` (patrz sekcjÄ™ "Konfiguracja")
2. Zakomentuj serwis `ollama` w `docker-compose.local.yml` (lub usuÅ„ `depends_on`)
3. Uruchom:
```bash
docker-compose -f docker-compose.local.yml up -d
```

**JeÅ›li potrzebujesz nowego kontenera Ollama:**
```bash
# UÅ¼yj specjalnego docker-compose dla trybu lokalnego
docker-compose -f docker-compose.local.yml up -d
```

## Pobieranie modeli Ollama

### Automatyczne

Modele sÄ… pobierane automatycznie przy pierwszym uÅ¼yciu. To moÅ¼e zajÄ…Ä‡ kilka minut.

### RÄ™czne

```bash
# PoÅ‚Ä…cz siÄ™ z kontenerem Ollama
docker exec -it paragon_ollama ollama pull llava:latest
docker exec -it paragon_ollama ollama pull SpeakLeash/bielik-11b-v2.3-instruct:Q4_K_M

# SprawdÅº pobrane modele
docker exec -it paragon_ollama ollama list
```

## Sprawdzanie statusu

### Czy Ollama dziaÅ‚a?

```bash
# SprawdÅº kontener
docker ps | grep ollama

# SprawdÅº logi
docker logs paragon_ollama

# SprawdÅº API
docker exec paragon_ollama curl http://localhost:11434/api/tags
```

### Czy ParagonWeb Å‚Ä…czy siÄ™ z Ollama?

```bash
# SprawdÅº logi ParagonWeb
docker logs paragon_ocr | grep -i ollama

# SprawdÅº konfiguracjÄ™
docker exec paragon_ocr env | grep OLLAMA
```

## Troubleshooting

### Problem: "Nie moÅ¼na poÅ‚Ä…czyÄ‡ siÄ™ z Ollama"

**RozwiÄ…zanie:**
1. SprawdÅº czy kontener Ollama dziaÅ‚a:
   ```bash
   docker ps | grep ollama
   ```

2. SprawdÅº czy sÄ… w tej samej sieci:
   ```bash
   docker network inspect paragonocr_default
   ```

3. SprawdÅº OLLAMA_HOST w ParagonWeb:
   ```bash
   docker exec paragon_ocr env | grep OLLAMA_HOST
   # Powinno byÄ‡: OLLAMA_HOST=http://ollama:11434
   ```

### Problem: "Model nie znaleziony"

**RozwiÄ…zanie:**
```bash
# Pobierz model rÄ™cznie
docker exec -it paragon_ollama ollama pull llava:latest

# SprawdÅº dostÄ™pne modele
docker exec -it paragon_ollama ollama list
```

### Problem: "Wolne przetwarzanie"

**RozwiÄ…zanie:**
- Ollama w Dockerze moÅ¼e byÄ‡ wolne na sÅ‚abszym sprzÄ™cie
- RozwaÅ¼ uÅ¼ycie trybu Cloud (OpenAI jest szybsze)
- Lub zwiÄ™ksz zasoby dla kontenera Ollama:
  ```yaml
  ollama:
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
  ```

## PrzeÅ‚Ä…czanie miÄ™dzy trybami

### Z Cloud na Lokalny

1. Zatrzymaj kontenery:
   ```bash
   docker-compose down
   ```

2. Uruchom z konfiguracjÄ… lokalnÄ…:
   ```bash
   docker-compose -f docker-compose.local.yml up -d
   ```

3. Upewnij siÄ™, Å¼e modele sÄ… pobrane (patrz wyÅ¼ej)

### Z Lokalnego na Cloud

1. Zatrzymaj kontenery:
   ```bash
   docker-compose -f docker-compose.local.yml down
   ```

2. Uruchom standardowy docker-compose:
   ```bash
   docker-compose up -d
   ```

## Volume dla modeli Ollama

Modele Ollama sÄ… przechowywane w volume `ollama_data`, wiÄ™c sÄ… zachowywane miÄ™dzy restartami:

```bash
# SprawdÅº volume
docker volume ls | grep ollama

# Backup modeli (opcjonalnie)
docker run --rm -v ollama_data:/data -v $(pwd):/backup alpine tar czf /backup/ollama_backup.tar.gz /data
```

## WydajnoÅ›Ä‡

### Zalecenia

- **Cloud (OpenAI):** Najszybsze, najlepsze dla produkcji
- **Lokalny (Ollama):** Wymaga mocnego sprzÄ™tu (GPU zalecane)
- **Hybrydowy:** Cloud OCR + Lokalny AI (kompromis)

### Zasoby dla Ollama

Minimalne:
- 4GB RAM
- 2 CPU cores
- 10GB miejsca na dysku (dla modeli)

Zalecane:
- 8GB+ RAM
- 4+ CPU cores
- GPU (CUDA) dla szybszego przetwarzania
- 20GB+ miejsca na dysku

## BezpieczeÅ„stwo

- Ollama w Dockerze jest izolowane od hosta
- Komunikacja miÄ™dzy kontenerami odbywa siÄ™ przez sieÄ‡ Docker (nie jest eksponowana na zewnÄ…trz)
- Port 11434 jest eksponowany tylko jeÅ›li chcesz uÅ¼yÄ‡ Ollama z hosta

## Przydatne komendy

```bash
# Restart Ollama
docker restart paragon_ollama

# WyczyÅ›Ä‡ cache Ollama
docker exec paragon_ollama ollama rm <model_name>

# SprawdÅº uÅ¼ycie zasobÃ³w
docker stats paragon_ollama

# Zobacz wszystkie modele
docker exec -it paragon_ollama ollama list

# UsuÅ„ wszystkie modele (ostroÅ¼nie!)
docker exec -it paragon_ollama ollama list | awk '{print $1}' | xargs -I {} docker exec paragon_ollama ollama rm {}
```

---

**Ostatnia aktualizacja:** 2025-11-23

